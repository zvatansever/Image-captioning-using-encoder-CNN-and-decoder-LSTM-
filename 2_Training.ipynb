{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** \n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.88s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1042/414113 [00:00<01:23, 4927.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:05<00:00, 6336.30it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 10          # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(encoder.embed.parameters())+list(decoder.parameters()) \n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.1101, Perplexity: 60.9527\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.4056, Perplexity: 81.90824\n",
      "Epoch [1/3], Step [300/41412], Loss: 3.9829, Perplexity: 53.67076\n",
      "Epoch [1/3], Step [400/41412], Loss: 3.7059, Perplexity: 40.68806\n",
      "Epoch [1/3], Step [500/41412], Loss: 3.2541, Perplexity: 25.8975\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.3208, Perplexity: 27.68147\n",
      "Epoch [1/3], Step [700/41412], Loss: 4.0262, Perplexity: 56.04952\n",
      "Epoch [1/3], Step [800/41412], Loss: 4.1236, Perplexity: 61.7799\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.4994, Perplexity: 33.09622\n",
      "Epoch [1/3], Step [1000/41412], Loss: 3.4623, Perplexity: 31.8893\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.3164, Perplexity: 27.5615\n",
      "Epoch [1/3], Step [1200/41412], Loss: 3.6987, Perplexity: 40.3953\n",
      "Epoch [1/3], Step [1300/41412], Loss: 3.9696, Perplexity: 52.9642\n",
      "Epoch [1/3], Step [1400/41412], Loss: 2.9887, Perplexity: 19.8602\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.6121, Perplexity: 37.0451\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.2943, Perplexity: 26.9578\n",
      "Epoch [1/3], Step [1700/41412], Loss: 3.1860, Perplexity: 24.1915\n",
      "Epoch [1/3], Step [1800/41412], Loss: 3.8426, Perplexity: 46.64828\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.0431, Perplexity: 20.96949\n",
      "Epoch [1/3], Step [2000/41412], Loss: 3.2365, Perplexity: 25.4446\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.3220, Perplexity: 27.71510\n",
      "Epoch [1/3], Step [2200/41412], Loss: 3.4795, Perplexity: 32.4444\n",
      "Epoch [1/3], Step [2300/41412], Loss: 3.2555, Perplexity: 25.9327\n",
      "Epoch [1/3], Step [2400/41412], Loss: 3.1385, Perplexity: 23.06917\n",
      "Epoch [1/3], Step [2500/41412], Loss: 3.0887, Perplexity: 21.9480\n",
      "Epoch [1/3], Step [2600/41412], Loss: 3.0005, Perplexity: 20.0950\n",
      "Epoch [1/3], Step [2700/41412], Loss: 3.1850, Perplexity: 24.1684\n",
      "Epoch [1/3], Step [2800/41412], Loss: 3.4871, Perplexity: 32.6898\n",
      "Epoch [1/3], Step [2900/41412], Loss: 3.2455, Perplexity: 25.6751\n",
      "Epoch [1/3], Step [3000/41412], Loss: 3.6255, Perplexity: 37.5445\n",
      "Epoch [1/3], Step [3100/41412], Loss: 2.8275, Perplexity: 16.9038\n",
      "Epoch [1/3], Step [3200/41412], Loss: 3.3746, Perplexity: 29.2125\n",
      "Epoch [1/3], Step [3300/41412], Loss: 2.8692, Perplexity: 17.6221\n",
      "Epoch [1/3], Step [3400/41412], Loss: 2.9111, Perplexity: 18.3763\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.8980, Perplexity: 18.1387\n",
      "Epoch [1/3], Step [3600/41412], Loss: 2.8428, Perplexity: 17.1634\n",
      "Epoch [1/3], Step [3700/41412], Loss: 3.1644, Perplexity: 23.6757\n",
      "Epoch [1/3], Step [3800/41412], Loss: 3.5995, Perplexity: 36.5812\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.7121, Perplexity: 15.0603\n",
      "Epoch [1/3], Step [4000/41412], Loss: 3.1244, Perplexity: 22.7453\n",
      "Epoch [1/3], Step [4100/41412], Loss: 2.8944, Perplexity: 18.0732\n",
      "Epoch [1/3], Step [4200/41412], Loss: 3.4499, Perplexity: 31.4987\n",
      "Epoch [1/3], Step [4300/41412], Loss: 3.4630, Perplexity: 31.9122\n",
      "Epoch [1/3], Step [4400/41412], Loss: 3.2707, Perplexity: 26.3298\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.8855, Perplexity: 17.9118\n",
      "Epoch [1/3], Step [4600/41412], Loss: 2.8829, Perplexity: 17.8656\n",
      "Epoch [1/3], Step [4700/41412], Loss: 3.5702, Perplexity: 35.52272\n",
      "Epoch [1/3], Step [4800/41412], Loss: 2.8371, Perplexity: 17.0655\n",
      "Epoch [1/3], Step [4900/41412], Loss: 3.7166, Perplexity: 41.1237\n",
      "Epoch [1/3], Step [5000/41412], Loss: 2.7022, Perplexity: 14.9119\n",
      "Epoch [1/3], Step [5100/41412], Loss: 3.0552, Perplexity: 21.22510\n",
      "Epoch [1/3], Step [5200/41412], Loss: 2.9911, Perplexity: 19.9079\n",
      "Epoch [1/3], Step [5300/41412], Loss: 3.1695, Perplexity: 23.7964\n",
      "Epoch [1/3], Step [5400/41412], Loss: 3.1299, Perplexity: 22.8707\n",
      "Epoch [1/3], Step [5500/41412], Loss: 2.6852, Perplexity: 14.6611\n",
      "Epoch [1/3], Step [5600/41412], Loss: 3.1714, Perplexity: 23.8399\n",
      "Epoch [1/3], Step [5700/41412], Loss: 2.4643, Perplexity: 11.7558\n",
      "Epoch [1/3], Step [5800/41412], Loss: 3.1844, Perplexity: 24.1526\n",
      "Epoch [1/3], Step [5900/41412], Loss: 3.5765, Perplexity: 35.7482\n",
      "Epoch [1/3], Step [6000/41412], Loss: 3.0502, Perplexity: 21.1194\n",
      "Epoch [1/3], Step [6100/41412], Loss: 2.5350, Perplexity: 12.6160\n",
      "Epoch [1/3], Step [6200/41412], Loss: 3.7241, Perplexity: 41.4320\n",
      "Epoch [1/3], Step [6300/41412], Loss: 2.8355, Perplexity: 17.0393\n",
      "Epoch [1/3], Step [6400/41412], Loss: 2.3159, Perplexity: 10.1343\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.3639, Perplexity: 10.6323\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.7337, Perplexity: 15.38949\n",
      "Epoch [1/3], Step [6700/41412], Loss: 2.3998, Perplexity: 11.0209\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.9584, Perplexity: 19.2665\n",
      "Epoch [1/3], Step [6900/41412], Loss: 2.5859, Perplexity: 13.2747\n",
      "Epoch [1/3], Step [7000/41412], Loss: 3.2391, Perplexity: 25.5119\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.3979, Perplexity: 11.0006\n",
      "Epoch [1/3], Step [7200/41412], Loss: 3.2194, Perplexity: 25.0125\n",
      "Epoch [1/3], Step [7300/41412], Loss: 2.1740, Perplexity: 8.79310\n",
      "Epoch [1/3], Step [7400/41412], Loss: 3.0039, Perplexity: 20.1650\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.1642, Perplexity: 8.70808\n",
      "Epoch [1/3], Step [7600/41412], Loss: 3.8180, Perplexity: 45.5141\n",
      "Epoch [1/3], Step [7700/41412], Loss: 3.3151, Perplexity: 27.5247\n",
      "Epoch [1/3], Step [7800/41412], Loss: 2.7824, Perplexity: 16.1582\n",
      "Epoch [1/3], Step [7900/41412], Loss: 2.6217, Perplexity: 13.7595\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.1400, Perplexity: 8.49923\n",
      "Epoch [1/3], Step [8100/41412], Loss: 2.4420, Perplexity: 11.4965\n",
      "Epoch [1/3], Step [8200/41412], Loss: 3.3232, Perplexity: 27.7495\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.8471, Perplexity: 17.2378\n",
      "Epoch [1/3], Step [8400/41412], Loss: 2.5294, Perplexity: 12.5463\n",
      "Epoch [1/3], Step [8500/41412], Loss: 3.1609, Perplexity: 23.5925\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.8848, Perplexity: 17.8999\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.6963, Perplexity: 14.8247\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.9107, Perplexity: 18.3702\n",
      "Epoch [1/3], Step [8900/41412], Loss: 2.3459, Perplexity: 10.4428\n",
      "Epoch [1/3], Step [9000/41412], Loss: 3.1238, Perplexity: 22.7334\n",
      "Epoch [1/3], Step [9100/41412], Loss: 2.2843, Perplexity: 9.81938\n",
      "Epoch [1/3], Step [9200/41412], Loss: 2.2453, Perplexity: 9.44305\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.8813, Perplexity: 17.8373\n",
      "Epoch [1/3], Step [9400/41412], Loss: 3.0054, Perplexity: 20.1940\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.2334, Perplexity: 9.33147\n",
      "Epoch [1/3], Step [9600/41412], Loss: 2.3446, Perplexity: 10.4290\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.5055, Perplexity: 12.2500\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.9473, Perplexity: 19.0551\n",
      "Epoch [1/3], Step [9900/41412], Loss: 2.3587, Perplexity: 10.5772\n",
      "Epoch [1/3], Step [10000/41412], Loss: 2.3353, Perplexity: 10.3327\n",
      "Epoch [1/3], Step [10100/41412], Loss: 2.7215, Perplexity: 15.2030\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.3150, Perplexity: 10.1252\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.7460, Perplexity: 15.5797\n",
      "Epoch [1/3], Step [10400/41412], Loss: 2.7035, Perplexity: 14.9316\n",
      "Epoch [1/3], Step [10500/41412], Loss: 2.8056, Perplexity: 16.5366\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.4119, Perplexity: 11.1546\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.7860, Perplexity: 16.2161\n",
      "Epoch [1/3], Step [10800/41412], Loss: 2.2353, Perplexity: 9.34885\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.9808, Perplexity: 19.7042\n",
      "Epoch [1/3], Step [11000/41412], Loss: 3.3348, Perplexity: 28.0720\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.7070, Perplexity: 14.9848\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.3985, Perplexity: 11.0064\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.5904, Perplexity: 13.3350\n",
      "Epoch [1/3], Step [11400/41412], Loss: 3.0323, Perplexity: 20.7445\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.3904, Perplexity: 10.9179\n",
      "Epoch [1/3], Step [11600/41412], Loss: 2.9541, Perplexity: 19.1844\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.6954, Perplexity: 14.8117\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.6933, Perplexity: 14.7801\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.7143, Perplexity: 15.0934\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.7183, Perplexity: 15.15468\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.6449, Perplexity: 14.0822\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.5882, Perplexity: 13.3060\n",
      "Epoch [1/3], Step [12300/41412], Loss: 2.4281, Perplexity: 11.3377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [12400/41412], Loss: 3.2267, Perplexity: 25.1963\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.7405, Perplexity: 15.4948\n",
      "Epoch [1/3], Step [12600/41412], Loss: 2.3200, Perplexity: 10.1759\n",
      "Epoch [1/3], Step [12700/41412], Loss: 2.7628, Perplexity: 15.8449\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.6115, Perplexity: 13.6199\n",
      "Epoch [1/3], Step [12900/41412], Loss: 3.2146, Perplexity: 24.8923\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.0399, Perplexity: 7.68998\n",
      "Epoch [1/3], Step [13100/41412], Loss: 2.3791, Perplexity: 10.7952\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.6570, Perplexity: 14.2531\n",
      "Epoch [1/3], Step [13300/41412], Loss: 3.1434, Perplexity: 23.1820\n",
      "Epoch [1/3], Step [13400/41412], Loss: 2.1021, Perplexity: 8.18313\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.8814, Perplexity: 17.8400\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.5676, Perplexity: 13.0342\n",
      "Epoch [1/3], Step [13700/41412], Loss: 2.5462, Perplexity: 12.7591\n",
      "Epoch [1/3], Step [13800/41412], Loss: 2.8492, Perplexity: 17.2740\n",
      "Epoch [1/3], Step [13900/41412], Loss: 2.2490, Perplexity: 9.478246\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.3768, Perplexity: 10.7707\n",
      "Epoch [1/3], Step [14100/41412], Loss: 2.0699, Perplexity: 7.92442\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.6321, Perplexity: 13.9035\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.1940, Perplexity: 8.97143\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.4469, Perplexity: 11.5521\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.3044, Perplexity: 10.0182\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.4013, Perplexity: 11.0377\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.7834, Perplexity: 16.1741\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.4689, Perplexity: 11.80979\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.2217, Perplexity: 9.22304\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.6146, Perplexity: 13.6617\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.6767, Perplexity: 14.5371\n",
      "Epoch [1/3], Step [15200/41412], Loss: 1.8722, Perplexity: 6.50271\n",
      "Epoch [1/3], Step [15300/41412], Loss: 2.5373, Perplexity: 12.6455\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.2622, Perplexity: 9.60388\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.7567, Perplexity: 15.7480\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.1893, Perplexity: 8.92896\n",
      "Epoch [1/3], Step [15700/41412], Loss: 2.6423, Perplexity: 14.0461\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.3910, Perplexity: 10.9243\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.2556, Perplexity: 9.54103\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.3329, Perplexity: 10.3082\n",
      "Epoch [1/3], Step [16100/41412], Loss: 2.7774, Perplexity: 16.0765\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.1068, Perplexity: 8.22200\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.8307, Perplexity: 16.9582\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.5109, Perplexity: 12.3156\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.8666, Perplexity: 17.5769\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.8919, Perplexity: 18.0269\n",
      "Epoch [1/3], Step [16700/41412], Loss: 3.2062, Perplexity: 24.6858\n",
      "Epoch [1/3], Step [16800/41412], Loss: 2.2025, Perplexity: 9.04733\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.4289, Perplexity: 11.3466\n",
      "Epoch [1/3], Step [17000/41412], Loss: 2.3349, Perplexity: 10.3284\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.9962, Perplexity: 20.0096\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.4747, Perplexity: 11.8779\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.7875, Perplexity: 16.2404\n",
      "Epoch [1/3], Step [17400/41412], Loss: 1.9550, Perplexity: 7.06370\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.8558, Perplexity: 17.3878\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.3232, Perplexity: 10.2084\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.1234, Perplexity: 8.35930\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.2915, Perplexity: 9.88971\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.7561, Perplexity: 15.73761\n",
      "Epoch [1/3], Step [18000/41412], Loss: 3.0190, Perplexity: 20.4713\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.1695, Perplexity: 8.75431\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.3874, Perplexity: 10.8851\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.8317, Perplexity: 16.9741\n",
      "Epoch [1/3], Step [18400/41412], Loss: 2.5621, Perplexity: 12.9632\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.5298, Perplexity: 12.5511\n",
      "Epoch [1/3], Step [18600/41412], Loss: 3.0469, Perplexity: 21.0507\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.0730, Perplexity: 7.94831\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.4068, Perplexity: 11.0987\n",
      "Epoch [1/3], Step [18900/41412], Loss: 2.5408, Perplexity: 12.6898\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.1727, Perplexity: 8.78167\n",
      "Epoch [1/3], Step [19100/41412], Loss: 1.9863, Perplexity: 7.28884\n",
      "Epoch [1/3], Step [19200/41412], Loss: 3.2916, Perplexity: 26.8852\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.7161, Perplexity: 15.1208\n",
      "Epoch [1/3], Step [19400/41412], Loss: 2.2930, Perplexity: 9.90423\n",
      "Epoch [1/3], Step [19500/41412], Loss: 2.2100, Perplexity: 9.11552\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.2722, Perplexity: 9.70047\n",
      "Epoch [1/3], Step [19700/41412], Loss: 2.4892, Perplexity: 12.0518\n",
      "Epoch [1/3], Step [19800/41412], Loss: 3.5301, Perplexity: 34.1285\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.3792, Perplexity: 10.7967\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.4296, Perplexity: 11.3542\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.4528, Perplexity: 11.6211\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.0732, Perplexity: 7.95013\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.3756, Perplexity: 10.75732\n",
      "Epoch [1/3], Step [20400/41412], Loss: 2.2432, Perplexity: 9.42305\n",
      "Epoch [1/3], Step [20500/41412], Loss: 3.0494, Perplexity: 21.1021\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.5993, Perplexity: 13.4543\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.7629, Perplexity: 15.8452\n",
      "Epoch [1/3], Step [20800/41412], Loss: 2.7286, Perplexity: 15.3114\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.3650, Perplexity: 10.6442\n",
      "Epoch [1/3], Step [21000/41412], Loss: 3.2147, Perplexity: 24.8955\n",
      "Epoch [1/3], Step [21100/41412], Loss: 2.2622, Perplexity: 9.60470\n",
      "Epoch [1/3], Step [21200/41412], Loss: 2.9743, Perplexity: 19.5751\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.4370, Perplexity: 11.4390\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.5112, Perplexity: 12.3194\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.4344, Perplexity: 11.4094\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.8177, Perplexity: 16.7376\n",
      "Epoch [1/3], Step [21700/41412], Loss: 2.4957, Perplexity: 12.1298\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.4935, Perplexity: 12.1039\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.5155, Perplexity: 12.3730\n",
      "Epoch [1/3], Step [22000/41412], Loss: 2.8782, Perplexity: 17.78243\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.1265, Perplexity: 8.38560\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.4129, Perplexity: 11.1665\n",
      "Epoch [1/3], Step [22300/41412], Loss: 2.8754, Perplexity: 17.7321\n",
      "Epoch [1/3], Step [22400/41412], Loss: 2.2906, Perplexity: 9.88081\n",
      "Epoch [1/3], Step [22500/41412], Loss: 3.0931, Perplexity: 22.0453\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.0860, Perplexity: 8.05252\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.4531, Perplexity: 11.6247\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.0558, Perplexity: 7.81323\n",
      "Epoch [1/3], Step [22900/41412], Loss: 2.0480, Perplexity: 7.75270\n",
      "Epoch [1/3], Step [23000/41412], Loss: 3.0547, Perplexity: 21.2140\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.3658, Perplexity: 10.6530\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.3313, Perplexity: 10.2915\n",
      "Epoch [1/3], Step [23300/41412], Loss: 1.9896, Perplexity: 7.31243\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.7781, Perplexity: 16.0878\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.3118, Perplexity: 10.0927\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.5045, Perplexity: 12.2370\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.1692, Perplexity: 8.75093\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.1585, Perplexity: 8.65837\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.5283, Perplexity: 12.5316\n",
      "Epoch [1/3], Step [24000/41412], Loss: 2.6767, Perplexity: 14.5372\n",
      "Epoch [1/3], Step [24100/41412], Loss: 3.2569, Perplexity: 25.9702\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.7785, Perplexity: 16.0952\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.6269, Perplexity: 13.8304\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.8277, Perplexity: 16.9065\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.5924, Perplexity: 13.3621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 1.4638, Perplexity: 4.32226\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.3258, Perplexity: 10.2346\n",
      "Epoch [1/3], Step [24800/41412], Loss: 1.7633, Perplexity: 5.83161\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.8746, Perplexity: 17.7182\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.3691, Perplexity: 10.6875\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.4099, Perplexity: 11.1328\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.4268, Perplexity: 11.3227\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.8277, Perplexity: 16.9063\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.1472, Perplexity: 8.56059\n",
      "Epoch [1/3], Step [25500/41412], Loss: 2.6860, Perplexity: 14.6724\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.2717, Perplexity: 9.69585\n",
      "Epoch [1/3], Step [25700/41412], Loss: 2.4952, Perplexity: 12.1246\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.8768, Perplexity: 17.7579\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.8737, Perplexity: 17.7023\n",
      "Epoch [1/3], Step [26000/41412], Loss: 3.1510, Perplexity: 23.3586\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.2648, Perplexity: 9.62960\n",
      "Epoch [1/3], Step [26200/41412], Loss: 1.9051, Perplexity: 6.72012\n",
      "Epoch [1/3], Step [26300/41412], Loss: 3.0425, Perplexity: 20.9586\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.6654, Perplexity: 14.3737\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.6069, Perplexity: 13.5576\n",
      "Epoch [1/3], Step [26600/41412], Loss: 1.7433, Perplexity: 5.71637\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.2769, Perplexity: 9.74601\n",
      "Epoch [1/3], Step [26800/41412], Loss: 1.9781, Perplexity: 7.22918\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.3461, Perplexity: 10.4450\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.7314, Perplexity: 15.3539\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.9150, Perplexity: 18.4492\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.2255, Perplexity: 9.25792\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.1533, Perplexity: 8.61360\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.0543, Perplexity: 7.80151\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.5145, Perplexity: 12.3601\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.2690, Perplexity: 9.66989\n",
      "Epoch [1/3], Step [27700/41412], Loss: 1.9717, Perplexity: 7.18319\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.1902, Perplexity: 8.93682\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.3972, Perplexity: 10.9928\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.4047, Perplexity: 11.0753\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.6061, Perplexity: 13.5466\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.5205, Perplexity: 12.4346\n",
      "Epoch [1/3], Step [28300/41412], Loss: 3.1497, Perplexity: 23.3291\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.1495, Perplexity: 8.58075\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.2515, Perplexity: 9.50197\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.3392, Perplexity: 10.3733\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.3097, Perplexity: 10.0713\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.1877, Perplexity: 8.914348\n",
      "Epoch [1/3], Step [28900/41412], Loss: 2.5612, Perplexity: 12.9516\n",
      "Epoch [1/3], Step [29000/41412], Loss: 2.7783, Perplexity: 16.0917\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.3208, Perplexity: 10.1837\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.2276, Perplexity: 9.27715\n",
      "Epoch [1/3], Step [29300/41412], Loss: 1.9439, Perplexity: 6.98608\n",
      "Epoch [1/3], Step [29400/41412], Loss: 2.9299, Perplexity: 18.7254\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.1592, Perplexity: 8.66409\n",
      "Epoch [1/3], Step [29600/41412], Loss: 2.0974, Perplexity: 8.14485\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.3283, Perplexity: 10.26073\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.8164, Perplexity: 16.7158\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.3499, Perplexity: 10.4845\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.9317, Perplexity: 18.7594\n",
      "Epoch [1/3], Step [30100/41412], Loss: 3.1492, Perplexity: 23.3179\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.3493, Perplexity: 10.4786\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.6574, Perplexity: 14.2594\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.5464, Perplexity: 12.7606\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.1545, Perplexity: 8.62351\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.6753, Perplexity: 14.5166\n",
      "Epoch [1/3], Step [30700/41412], Loss: 1.9658, Perplexity: 7.14089\n",
      "Epoch [1/3], Step [30800/41412], Loss: 1.8888, Perplexity: 6.61132\n",
      "Epoch [1/3], Step [30900/41412], Loss: 1.9347, Perplexity: 6.92172\n",
      "Epoch [1/3], Step [31000/41412], Loss: 2.6196, Perplexity: 13.7297\n",
      "Epoch [1/3], Step [31100/41412], Loss: 2.3594, Perplexity: 10.5844\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.5942, Perplexity: 13.3864\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.7588, Perplexity: 15.7803\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.1841, Perplexity: 8.88271\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.1286, Perplexity: 8.40358\n",
      "Epoch [1/3], Step [31600/41412], Loss: 2.8321, Perplexity: 16.9816\n",
      "Epoch [1/3], Step [31700/41412], Loss: 2.2991, Perplexity: 9.96539\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.2746, Perplexity: 9.72407\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.5529, Perplexity: 12.8442\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.2972, Perplexity: 9.94619\n",
      "Epoch [1/3], Step [32100/41412], Loss: 2.3933, Perplexity: 10.9501\n",
      "Epoch [1/3], Step [32200/41412], Loss: 1.8653, Perplexity: 6.45819\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.7038, Perplexity: 14.9359\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.5348, Perplexity: 12.6144\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.5110, Perplexity: 12.3170\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.0636, Perplexity: 7.87412\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.6630, Perplexity: 14.33867\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.2258, Perplexity: 9.26049\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.4598, Perplexity: 11.7024\n",
      "Epoch [1/3], Step [33000/41412], Loss: 2.3817, Perplexity: 10.8236\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.4022, Perplexity: 11.0474\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.6818, Perplexity: 14.6116\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.1595, Perplexity: 8.66721\n",
      "Epoch [1/3], Step [33400/41412], Loss: 2.7332, Perplexity: 15.3826\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.0008, Perplexity: 7.39539\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.3789, Perplexity: 10.7925\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.3137, Perplexity: 10.1114\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.1474, Perplexity: 8.56291\n",
      "Epoch [1/3], Step [33900/41412], Loss: 1.7901, Perplexity: 5.98997\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.8769, Perplexity: 17.7583\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.5251, Perplexity: 12.4916\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.5974, Perplexity: 13.4294\n",
      "Epoch [1/3], Step [34300/41412], Loss: 2.2763, Perplexity: 9.74027\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.1293, Perplexity: 8.408805\n",
      "Epoch [1/3], Step [34500/41412], Loss: 2.4269, Perplexity: 11.3239\n",
      "Epoch [1/3], Step [34600/41412], Loss: 2.7420, Perplexity: 15.5173\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.8814, Perplexity: 17.8389\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.1617, Perplexity: 8.68613\n",
      "Epoch [1/3], Step [34900/41412], Loss: 2.3301, Perplexity: 10.2793\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.2278, Perplexity: 9.27973\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.1434, Perplexity: 8.52807\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.2783, Perplexity: 9.76049\n",
      "Epoch [1/3], Step [35300/41412], Loss: 2.4298, Perplexity: 11.3564\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.1686, Perplexity: 8.74644\n",
      "Epoch [1/3], Step [35500/41412], Loss: 3.0359, Perplexity: 20.8206\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.6238, Perplexity: 13.7884\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.4123, Perplexity: 11.1597\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.4016, Perplexity: 11.0413\n",
      "Epoch [1/3], Step [35900/41412], Loss: 2.3687, Perplexity: 10.6833\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.7355, Perplexity: 15.4181\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.1702, Perplexity: 8.76025\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.4800, Perplexity: 11.9415\n",
      "Epoch [1/3], Step [36300/41412], Loss: 1.9463, Perplexity: 7.00311\n",
      "Epoch [1/3], Step [36400/41412], Loss: 1.7825, Perplexity: 5.94480\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.6283, Perplexity: 13.8507\n",
      "Epoch [1/3], Step [36600/41412], Loss: 1.8576, Perplexity: 6.40822\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.1575, Perplexity: 8.64928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [36800/41412], Loss: 2.0356, Perplexity: 7.65650\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.1624, Perplexity: 8.69228\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.1704, Perplexity: 8.76164\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.4682, Perplexity: 11.8010\n",
      "Epoch [1/3], Step [37200/41412], Loss: 2.4146, Perplexity: 11.1853\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.3960, Perplexity: 10.9796\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.1915, Perplexity: 8.94825\n",
      "Epoch [1/3], Step [37500/41412], Loss: 1.8753, Perplexity: 6.52296\n",
      "Epoch [1/3], Step [37600/41412], Loss: 2.5919, Perplexity: 13.3555\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.3105, Perplexity: 10.0792\n",
      "Epoch [1/3], Step [37800/41412], Loss: 1.9145, Perplexity: 6.78355\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.2642, Perplexity: 9.62378\n",
      "Epoch [1/3], Step [38000/41412], Loss: 2.9217, Perplexity: 18.5725\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.1847, Perplexity: 8.88794\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.1492, Perplexity: 8.57841\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.9300, Perplexity: 18.7272\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.9518, Perplexity: 19.1404\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.3721, Perplexity: 10.7197\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.3916, Perplexity: 10.9315\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.7102, Perplexity: 15.0326\n",
      "Epoch [1/3], Step [38800/41412], Loss: 1.8830, Perplexity: 6.57315\n",
      "Epoch [1/3], Step [38900/41412], Loss: 1.9163, Perplexity: 6.79562\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.0974, Perplexity: 8.14470\n",
      "Epoch [1/3], Step [39100/41412], Loss: 3.1045, Perplexity: 22.2988\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.5290, Perplexity: 12.5407\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.1947, Perplexity: 8.97731\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.3776, Perplexity: 10.7793\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.0971, Perplexity: 8.14261\n",
      "Epoch [1/3], Step [39600/41412], Loss: 1.9332, Perplexity: 6.91135\n",
      "Epoch [1/3], Step [39700/41412], Loss: 2.6334, Perplexity: 13.9217\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.4693, Perplexity: 11.8140\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.4103, Perplexity: 11.1372\n",
      "Epoch [1/3], Step [40000/41412], Loss: 2.3585, Perplexity: 10.5753\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.6373, Perplexity: 13.9758\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.2526, Perplexity: 9.51266\n",
      "Epoch [1/3], Step [40300/41412], Loss: 2.8316, Perplexity: 16.9734\n",
      "Epoch [1/3], Step [40400/41412], Loss: 2.6624, Perplexity: 14.3312\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.5459, Perplexity: 12.7544\n",
      "Epoch [1/3], Step [40600/41412], Loss: 2.3972, Perplexity: 10.9925\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.4243, Perplexity: 11.2949\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.6408, Perplexity: 14.0249\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.3586, Perplexity: 10.5765\n",
      "Epoch [1/3], Step [41000/41412], Loss: 1.9452, Perplexity: 6.99528\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.4271, Perplexity: 11.3255\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.3653, Perplexity: 10.6474\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.2044, Perplexity: 9.06471\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.1500, Perplexity: 8.58487\n",
      "Epoch [2/3], Step [100/41412], Loss: 2.1505, Perplexity: 8.5892022\n",
      "Epoch [2/3], Step [200/41412], Loss: 1.9988, Perplexity: 7.38067\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.7544, Perplexity: 15.7119\n",
      "Epoch [2/3], Step [400/41412], Loss: 2.2906, Perplexity: 9.88100\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.2926, Perplexity: 9.90102\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.4197, Perplexity: 11.2428\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.5447, Perplexity: 12.7398\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.3659, Perplexity: 10.6534\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.6796, Perplexity: 14.5789\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.6047, Perplexity: 13.5267\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.5171, Perplexity: 12.3930\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.1933, Perplexity: 8.96455\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.1188, Perplexity: 8.32133\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.2941, Perplexity: 9.91537\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.8023, Perplexity: 16.4832\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.6678, Perplexity: 14.4086\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.5362, Perplexity: 12.6311\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.5416, Perplexity: 12.7003\n",
      "Epoch [2/3], Step [1900/41412], Loss: 1.9295, Perplexity: 6.88612\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.6253, Perplexity: 13.8093\n",
      "Epoch [2/3], Step [2100/41412], Loss: 2.5507, Perplexity: 12.8156\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.5155, Perplexity: 12.3731\n",
      "Epoch [2/3], Step [2300/41412], Loss: 1.9925, Perplexity: 7.33380\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.2361, Perplexity: 9.35640\n",
      "Epoch [2/3], Step [2500/41412], Loss: 1.8108, Perplexity: 6.11513\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.4172, Perplexity: 11.2149\n",
      "Epoch [2/3], Step [2700/41412], Loss: 2.1614, Perplexity: 8.68340\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.2820, Perplexity: 9.79616\n",
      "Epoch [2/3], Step [2900/41412], Loss: 2.8799, Perplexity: 17.8119\n",
      "Epoch [2/3], Step [3000/41412], Loss: 2.4466, Perplexity: 11.5495\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.4433, Perplexity: 11.5115\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.2674, Perplexity: 9.65450\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.0374, Perplexity: 7.67048\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.8550, Perplexity: 17.3736\n",
      "Epoch [2/3], Step [3500/41412], Loss: 2.4600, Perplexity: 11.7052\n",
      "Epoch [2/3], Step [3600/41412], Loss: 2.4051, Perplexity: 11.0798\n",
      "Epoch [2/3], Step [3700/41412], Loss: 1.9604, Perplexity: 7.10214\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.7642, Perplexity: 15.8669\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.6239, Perplexity: 13.7894\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.3585, Perplexity: 10.5755\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.1301, Perplexity: 8.41616\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.0847, Perplexity: 8.04247\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.6218, Perplexity: 13.7601\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.4780, Perplexity: 11.9169\n",
      "Epoch [2/3], Step [4500/41412], Loss: 2.2547, Perplexity: 9.53270\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.4773, Perplexity: 11.9088\n",
      "Epoch [2/3], Step [4700/41412], Loss: 1.8465, Perplexity: 6.33760\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.5160, Perplexity: 12.3785\n",
      "Epoch [2/3], Step [4900/41412], Loss: 2.3406, Perplexity: 10.3875\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.8398, Perplexity: 17.1120\n",
      "Epoch [2/3], Step [5100/41412], Loss: 2.0749, Perplexity: 7.96387\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.5452, Perplexity: 12.7451\n",
      "Epoch [2/3], Step [5300/41412], Loss: 1.9799, Perplexity: 7.24201\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.4011, Perplexity: 11.0348\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.7708, Perplexity: 15.9709\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.7704, Perplexity: 15.9646\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.3239, Perplexity: 10.2151\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.3558, Perplexity: 10.5464\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.2597, Perplexity: 9.58061\n",
      "Epoch [2/3], Step [6000/41412], Loss: 2.7206, Perplexity: 15.1899\n",
      "Epoch [2/3], Step [6100/41412], Loss: 2.1119, Perplexity: 8.26417\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.2111, Perplexity: 9.12584\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.0229, Perplexity: 7.56002\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.6494, Perplexity: 14.1456\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.5342, Perplexity: 12.6058\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.3075, Perplexity: 10.0495\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.2344, Perplexity: 9.34090\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.1929, Perplexity: 8.96084\n",
      "Epoch [2/3], Step [6900/41412], Loss: 3.4951, Perplexity: 32.9522\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.0842, Perplexity: 8.03803\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.6221, Perplexity: 13.7647\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.0559, Perplexity: 7.81391\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.7212, Perplexity: 15.1988\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.3512, Perplexity: 10.4984\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.0947, Perplexity: 8.12346\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.2643, Perplexity: 9.62426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 1.8756, Perplexity: 6.52478\n",
      "Epoch [2/3], Step [7800/41412], Loss: 2.3996, Perplexity: 11.0190\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.3059, Perplexity: 10.0337\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.0283, Perplexity: 7.60142\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.6342, Perplexity: 13.9315\n",
      "Epoch [2/3], Step [8200/41412], Loss: 2.2367, Perplexity: 9.36201\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.3081, Perplexity: 10.0548\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.7772, Perplexity: 16.0746\n",
      "Epoch [2/3], Step [8500/41412], Loss: 1.9147, Perplexity: 6.78493\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.3370, Perplexity: 10.3505\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.2068, Perplexity: 9.08694\n",
      "Epoch [2/3], Step [8800/41412], Loss: 2.3211, Perplexity: 10.1869\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.4454, Perplexity: 11.5356\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.3753, Perplexity: 10.7540\n",
      "Epoch [2/3], Step [9100/41412], Loss: 3.0571, Perplexity: 21.2660\n",
      "Epoch [2/3], Step [9200/41412], Loss: 1.8725, Perplexity: 6.50432\n",
      "Epoch [2/3], Step [9300/41412], Loss: 2.6058, Perplexity: 13.5414\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.3919, Perplexity: 10.9346\n",
      "Epoch [2/3], Step [9500/41412], Loss: 2.1483, Perplexity: 8.57027\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.2275, Perplexity: 9.27710\n",
      "Epoch [2/3], Step [9700/41412], Loss: 2.4259, Perplexity: 11.3124\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.2228, Perplexity: 9.23317\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.3430, Perplexity: 10.4120\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.0533, Perplexity: 7.7932\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.5566, Perplexity: 12.8918\n",
      "Epoch [2/3], Step [10200/41412], Loss: 1.9357, Perplexity: 6.92915\n",
      "Epoch [2/3], Step [10300/41412], Loss: 1.9937, Perplexity: 7.34306\n",
      "Epoch [2/3], Step [10400/41412], Loss: 1.6352, Perplexity: 5.13038\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.2904, Perplexity: 9.87899\n",
      "Epoch [2/3], Step [10600/41412], Loss: 1.7016, Perplexity: 5.48306\n",
      "Epoch [2/3], Step [10700/41412], Loss: 2.2958, Perplexity: 9.93234\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.8325, Perplexity: 16.9881\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.3402, Perplexity: 10.3829\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.1228, Perplexity: 8.35447\n",
      "Epoch [2/3], Step [11100/41412], Loss: 2.1837, Perplexity: 8.87892\n",
      "Epoch [2/3], Step [11200/41412], Loss: 3.3900, Perplexity: 29.6655\n",
      "Epoch [2/3], Step [11300/41412], Loss: 2.5567, Perplexity: 12.8935\n",
      "Epoch [2/3], Step [11400/41412], Loss: 1.9916, Perplexity: 7.32703\n",
      "Epoch [2/3], Step [11500/41412], Loss: 2.4171, Perplexity: 11.2136\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.9861, Perplexity: 19.8086\n",
      "Epoch [2/3], Step [11700/41412], Loss: 1.9430, Perplexity: 6.97933\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.4365, Perplexity: 11.4330\n",
      "Epoch [2/3], Step [11900/41412], Loss: 2.0999, Perplexity: 8.16507\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.2127, Perplexity: 9.14079\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.4012, Perplexity: 11.0363\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.3109, Perplexity: 10.08313\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.0577, Perplexity: 7.82815\n",
      "Epoch [2/3], Step [12400/41412], Loss: 2.6266, Perplexity: 13.8273\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.6047, Perplexity: 13.5269\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.2579, Perplexity: 9.56262\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.1859, Perplexity: 8.89864\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.4354, Perplexity: 11.4201\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.8364, Perplexity: 17.0548\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.2134, Perplexity: 9.14647\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.4098, Perplexity: 11.1317\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.3021, Perplexity: 9.99532\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.0748, Perplexity: 7.96319\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.7700, Perplexity: 15.9582\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.1650, Perplexity: 8.71508\n",
      "Epoch [2/3], Step [13600/41412], Loss: 3.2243, Perplexity: 25.1348\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.0123, Perplexity: 7.48068\n",
      "Epoch [2/3], Step [13800/41412], Loss: 1.8283, Perplexity: 6.22347\n",
      "Epoch [2/3], Step [13900/41412], Loss: 1.7607, Perplexity: 5.81671\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.6525, Perplexity: 14.1892\n",
      "Epoch [2/3], Step [14100/41412], Loss: 2.1031, Perplexity: 8.19126\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.2691, Perplexity: 9.67086\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.0368, Perplexity: 7.66643\n",
      "Epoch [2/3], Step [14400/41412], Loss: 1.7058, Perplexity: 5.505928\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.4596, Perplexity: 11.7007\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.2866, Perplexity: 9.84185\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.6800, Perplexity: 14.5844\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.2986, Perplexity: 9.96016\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.2462, Perplexity: 9.45184\n",
      "Epoch [2/3], Step [15000/41412], Loss: 2.4623, Perplexity: 11.7319\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.4147, Perplexity: 11.1867\n",
      "Epoch [2/3], Step [15200/41412], Loss: 2.3244, Perplexity: 10.2207\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.4997, Perplexity: 12.1786\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.5812, Perplexity: 13.2126\n",
      "Epoch [2/3], Step [15500/41412], Loss: 3.0139, Perplexity: 20.3657\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.3376, Perplexity: 10.3559\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.5163, Perplexity: 12.3832\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.5684, Perplexity: 13.0452\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.6638, Perplexity: 14.3507\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.3579, Perplexity: 10.5689\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.2150, Perplexity: 9.16109\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.1686, Perplexity: 8.74595\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.4164, Perplexity: 11.2051\n",
      "Epoch [2/3], Step [16400/41412], Loss: 1.9010, Perplexity: 6.69279\n",
      "Epoch [2/3], Step [16500/41412], Loss: 1.9810, Perplexity: 7.24986\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.2635, Perplexity: 9.61635\n",
      "Epoch [2/3], Step [16700/41412], Loss: 1.7452, Perplexity: 5.72718\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.5648, Perplexity: 12.9982\n",
      "Epoch [2/3], Step [16900/41412], Loss: 3.2282, Perplexity: 25.2352\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.2515, Perplexity: 9.50209\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.3594, Perplexity: 10.5847\n",
      "Epoch [2/3], Step [17200/41412], Loss: 2.4191, Perplexity: 11.2355\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.4923, Perplexity: 12.0888\n",
      "Epoch [2/3], Step [17400/41412], Loss: 2.4752, Perplexity: 11.8840\n",
      "Epoch [2/3], Step [17500/41412], Loss: 2.7019, Perplexity: 14.9076\n",
      "Epoch [2/3], Step [17600/41412], Loss: 1.9997, Perplexity: 7.38665\n",
      "Epoch [2/3], Step [17700/41412], Loss: 2.3256, Perplexity: 10.2333\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.9573, Perplexity: 19.2451\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.5348, Perplexity: 12.6143\n",
      "Epoch [2/3], Step [18000/41412], Loss: 2.7015, Perplexity: 14.9017\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.1532, Perplexity: 8.61274\n",
      "Epoch [2/3], Step [18200/41412], Loss: 2.6187, Perplexity: 13.7178\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.2706, Perplexity: 9.68570\n",
      "Epoch [2/3], Step [18400/41412], Loss: 1.6901, Perplexity: 5.42007\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.3396, Perplexity: 10.3769\n",
      "Epoch [2/3], Step [18600/41412], Loss: 1.9816, Perplexity: 7.25449\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.4492, Perplexity: 11.5786\n",
      "Epoch [2/3], Step [18800/41412], Loss: 2.0932, Perplexity: 8.11110\n",
      "Epoch [2/3], Step [18900/41412], Loss: 2.8831, Perplexity: 17.8692\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.2148, Perplexity: 9.15943\n",
      "Epoch [2/3], Step [19100/41412], Loss: 1.8993, Perplexity: 6.68096\n",
      "Epoch [2/3], Step [19200/41412], Loss: 2.0850, Perplexity: 8.04430\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.3021, Perplexity: 9.99529\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.6664, Perplexity: 14.3883\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.0839, Perplexity: 8.03542\n",
      "Epoch [2/3], Step [19600/41412], Loss: 2.7643, Perplexity: 15.8678\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.4862, Perplexity: 12.0160\n",
      "Epoch [2/3], Step [19800/41412], Loss: 2.1398, Perplexity: 8.49787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [19900/41412], Loss: 2.4191, Perplexity: 11.2352\n",
      "Epoch [2/3], Step [20000/41412], Loss: 2.1244, Perplexity: 8.36836\n",
      "Epoch [2/3], Step [20100/41412], Loss: 2.2922, Perplexity: 9.89666\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.7331, Perplexity: 15.3805\n",
      "Epoch [2/3], Step [20300/41412], Loss: 1.9287, Perplexity: 6.88084\n",
      "Epoch [2/3], Step [20400/41412], Loss: 1.8648, Perplexity: 6.45469\n",
      "Epoch [2/3], Step [20500/41412], Loss: 2.6151, Perplexity: 13.6690\n",
      "Epoch [2/3], Step [20600/41412], Loss: 2.5757, Perplexity: 13.1400\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.4807, Perplexity: 11.9494\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.3695, Perplexity: 10.6915\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.6489, Perplexity: 14.1385\n",
      "Epoch [2/3], Step [21000/41412], Loss: 1.9496, Perplexity: 7.026215\n",
      "Epoch [2/3], Step [21100/41412], Loss: 2.0278, Perplexity: 7.59775\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.3698, Perplexity: 10.6954\n",
      "Epoch [2/3], Step [21300/41412], Loss: 1.9958, Perplexity: 7.35781\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.5465, Perplexity: 12.7622\n",
      "Epoch [2/3], Step [21500/41412], Loss: 2.3508, Perplexity: 10.4944\n",
      "Epoch [2/3], Step [21600/41412], Loss: 2.8696, Perplexity: 17.6295\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.3871, Perplexity: 10.8817\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.6208, Perplexity: 13.7462\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.2587, Perplexity: 9.57056\n",
      "Epoch [2/3], Step [22000/41412], Loss: 1.9659, Perplexity: 7.14159\n",
      "Epoch [2/3], Step [22100/41412], Loss: 2.6784, Perplexity: 14.5618\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.4343, Perplexity: 11.4075\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.0224, Perplexity: 7.55631\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.6038, Perplexity: 13.5151\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.6100, Perplexity: 13.5986\n",
      "Epoch [2/3], Step [22600/41412], Loss: 1.9464, Perplexity: 7.00348\n",
      "Epoch [2/3], Step [22700/41412], Loss: 2.0354, Perplexity: 7.65526\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.5765, Perplexity: 13.1515\n",
      "Epoch [2/3], Step [22900/41412], Loss: 1.8175, Perplexity: 6.15659\n",
      "Epoch [2/3], Step [23000/41412], Loss: 1.6489, Perplexity: 5.20144\n",
      "Epoch [2/3], Step [23100/41412], Loss: 2.1674, Perplexity: 8.73520\n",
      "Epoch [2/3], Step [23200/41412], Loss: 2.1656, Perplexity: 8.72009\n",
      "Epoch [2/3], Step [23300/41412], Loss: 2.4120, Perplexity: 11.1558\n",
      "Epoch [2/3], Step [23400/41412], Loss: 2.7712, Perplexity: 15.9772\n",
      "Epoch [2/3], Step [23500/41412], Loss: 1.7503, Perplexity: 5.75634\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.4318, Perplexity: 11.3796\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.2127, Perplexity: 9.14046\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.3761, Perplexity: 10.7626\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.0189, Perplexity: 7.53010\n",
      "Epoch [2/3], Step [24000/41412], Loss: 2.4447, Perplexity: 11.5269\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.1907, Perplexity: 8.94186\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.4191, Perplexity: 11.2355\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.3758, Perplexity: 10.7599\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.3403, Perplexity: 10.3843\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.4417, Perplexity: 11.4920\n",
      "Epoch [2/3], Step [24600/41412], Loss: 1.9859, Perplexity: 7.28571\n",
      "Epoch [2/3], Step [24700/41412], Loss: 1.9330, Perplexity: 6.90994\n",
      "Epoch [2/3], Step [24800/41412], Loss: 2.3411, Perplexity: 10.3927\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.6404, Perplexity: 14.0194\n",
      "Epoch [2/3], Step [25000/41412], Loss: 2.5978, Perplexity: 13.4348\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.5069, Perplexity: 12.2666\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.1999, Perplexity: 9.023874\n",
      "Epoch [2/3], Step [25300/41412], Loss: 2.3577, Perplexity: 10.5663\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.1133, Perplexity: 8.275767\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.4162, Perplexity: 11.2032\n",
      "Epoch [2/3], Step [25600/41412], Loss: 1.9484, Perplexity: 7.01756\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.2958, Perplexity: 9.93225\n",
      "Epoch [2/3], Step [25800/41412], Loss: 2.7991, Perplexity: 16.4294\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.7616, Perplexity: 15.8246\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.7425, Perplexity: 15.5264\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.4483, Perplexity: 11.5690\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.6113, Perplexity: 13.6165\n",
      "Epoch [2/3], Step [26300/41412], Loss: 1.9916, Perplexity: 7.32752\n",
      "Epoch [2/3], Step [26400/41412], Loss: 2.5745, Perplexity: 13.1244\n",
      "Epoch [2/3], Step [26500/41412], Loss: 3.1005, Perplexity: 22.2088\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.3003, Perplexity: 9.97716\n",
      "Epoch [2/3], Step [26700/41412], Loss: 2.3484, Perplexity: 10.4692\n",
      "Epoch [2/3], Step [26800/41412], Loss: 2.3582, Perplexity: 10.5723\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.4550, Perplexity: 11.6467\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.1685, Perplexity: 8.74491\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.5747, Perplexity: 13.1268\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.8684, Perplexity: 17.6087\n",
      "Epoch [2/3], Step [27300/41412], Loss: 1.9463, Perplexity: 7.00241\n",
      "Epoch [2/3], Step [27400/41412], Loss: 2.2658, Perplexity: 9.63870\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.4526, Perplexity: 11.6180\n",
      "Epoch [2/3], Step [27600/41412], Loss: 2.1463, Perplexity: 8.55282\n",
      "Epoch [2/3], Step [27700/41412], Loss: 1.9827, Perplexity: 7.26207\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.2987, Perplexity: 9.96086\n",
      "Epoch [2/3], Step [27900/41412], Loss: 2.2000, Perplexity: 9.02537\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.2965, Perplexity: 9.93916\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.8965, Perplexity: 18.1100\n",
      "Epoch [2/3], Step [28200/41412], Loss: 1.9863, Perplexity: 7.28845\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.2010, Perplexity: 9.03387\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.3817, Perplexity: 10.8228\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.0519, Perplexity: 7.78267\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.5993, Perplexity: 13.4548\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.5644, Perplexity: 12.9931\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.7617, Perplexity: 15.8264\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.1808, Perplexity: 8.85358\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.1170, Perplexity: 8.30636\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.2007, Perplexity: 9.03152\n",
      "Epoch [2/3], Step [29200/41412], Loss: 1.9083, Perplexity: 6.74139\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.1232, Perplexity: 8.35810\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.2653, Perplexity: 9.63386\n",
      "Epoch [2/3], Step [29500/41412], Loss: 2.9921, Perplexity: 19.9285\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.7441, Perplexity: 15.5505\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.4893, Perplexity: 12.0529\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.3928, Perplexity: 10.9442\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.4667, Perplexity: 11.7841\n",
      "Epoch [2/3], Step [30000/41412], Loss: 1.8248, Perplexity: 6.20133\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.6167, Perplexity: 13.6898\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.2805, Perplexity: 9.78160\n",
      "Epoch [2/3], Step [30300/41412], Loss: 2.0818, Perplexity: 8.01933\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.9038, Perplexity: 18.2429\n",
      "Epoch [2/3], Step [30500/41412], Loss: 2.4121, Perplexity: 11.1579\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.1189, Perplexity: 8.32233\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.0951, Perplexity: 8.12607\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.6856, Perplexity: 14.6667\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.2186, Perplexity: 9.19447\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.8574, Perplexity: 17.4167\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.4859, Perplexity: 12.0115\n",
      "Epoch [2/3], Step [31200/41412], Loss: 1.8144, Perplexity: 6.13744\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.1555, Perplexity: 8.63216\n",
      "Epoch [2/3], Step [31400/41412], Loss: 2.5102, Perplexity: 12.3079\n",
      "Epoch [2/3], Step [31500/41412], Loss: 2.4222, Perplexity: 11.2706\n",
      "Epoch [2/3], Step [31600/41412], Loss: 2.4548, Perplexity: 11.6442\n",
      "Epoch [2/3], Step [31700/41412], Loss: 1.9107, Perplexity: 6.75809\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.5561, Perplexity: 12.8851\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.3069, Perplexity: 10.0427\n",
      "Epoch [2/3], Step [32000/41412], Loss: 1.7413, Perplexity: 5.70494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 2.2187, Perplexity: 9.19551\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.4468, Perplexity: 11.5517\n",
      "Epoch [2/3], Step [32300/41412], Loss: 1.8178, Perplexity: 6.15828\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.9839, Perplexity: 19.7653\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.2296, Perplexity: 9.29572\n",
      "Epoch [2/3], Step [32600/41412], Loss: 2.5647, Perplexity: 12.9963\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.7110, Perplexity: 15.0437\n",
      "Epoch [2/3], Step [32800/41412], Loss: 1.9641, Perplexity: 7.12866\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.6968, Perplexity: 14.8325\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.2552, Perplexity: 9.53716\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.1756, Perplexity: 8.80785\n",
      "Epoch [2/3], Step [33200/41412], Loss: 1.9340, Perplexity: 6.91696\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.6890, Perplexity: 14.7171\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.2302, Perplexity: 9.30214\n",
      "Epoch [2/3], Step [33500/41412], Loss: 2.3722, Perplexity: 10.7214\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.4487, Perplexity: 11.5736\n",
      "Epoch [2/3], Step [33700/41412], Loss: 1.7537, Perplexity: 5.776241\n",
      "Epoch [2/3], Step [33800/41412], Loss: 2.3056, Perplexity: 10.0297\n",
      "Epoch [2/3], Step [33900/41412], Loss: 1.8404, Perplexity: 6.29923\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.1177, Perplexity: 8.31206\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.3782, Perplexity: 10.7855\n",
      "Epoch [2/3], Step [34200/41412], Loss: 1.9683, Perplexity: 7.15853\n",
      "Epoch [2/3], Step [34300/41412], Loss: 1.8133, Perplexity: 6.13055\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.0503, Perplexity: 7.77008\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.0003, Perplexity: 7.39161\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.8973, Perplexity: 18.1249\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.3855, Perplexity: 10.8641\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.7953, Perplexity: 16.3683\n",
      "Epoch [2/3], Step [34900/41412], Loss: 1.8943, Perplexity: 6.64779\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.7885, Perplexity: 16.2569\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.1288, Perplexity: 8.40488\n",
      "Epoch [2/3], Step [35200/41412], Loss: 2.7004, Perplexity: 14.8852\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.2154, Perplexity: 9.16519\n",
      "Epoch [2/3], Step [35400/41412], Loss: 1.9311, Perplexity: 6.89705\n",
      "Epoch [2/3], Step [35500/41412], Loss: 1.7205, Perplexity: 5.58731\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.2746, Perplexity: 9.72362\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.1511, Perplexity: 8.59439\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.0264, Perplexity: 7.58694\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.3034, Perplexity: 10.0083\n",
      "Epoch [2/3], Step [36000/41412], Loss: 2.6835, Perplexity: 14.6369\n",
      "Epoch [2/3], Step [36100/41412], Loss: 1.9982, Perplexity: 7.37576\n",
      "Epoch [2/3], Step [36200/41412], Loss: 2.5067, Perplexity: 12.2649\n",
      "Epoch [2/3], Step [36300/41412], Loss: 1.8043, Perplexity: 6.07576\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.1870, Perplexity: 8.90832\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.4120, Perplexity: 11.1564\n",
      "Epoch [2/3], Step [36600/41412], Loss: 1.9571, Perplexity: 7.07883\n",
      "Epoch [2/3], Step [36700/41412], Loss: 3.3249, Perplexity: 27.7960\n",
      "Epoch [2/3], Step [36800/41412], Loss: 3.3663, Perplexity: 28.9701\n",
      "Epoch [2/3], Step [36900/41412], Loss: 2.6602, Perplexity: 14.2996\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.4903, Perplexity: 12.0650\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.2345, Perplexity: 9.34223\n",
      "Epoch [2/3], Step [37200/41412], Loss: 2.1577, Perplexity: 8.65153\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.0731, Perplexity: 7.94954\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.4631, Perplexity: 11.7414\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.4383, Perplexity: 11.4532\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.0765, Perplexity: 7.97671\n",
      "Epoch [2/3], Step [37700/41412], Loss: 1.6606, Perplexity: 5.26250\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.2400, Perplexity: 9.39303\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.3984, Perplexity: 11.0057\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.5631, Perplexity: 12.9766\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.3045, Perplexity: 10.0189\n",
      "Epoch [2/3], Step [38200/41412], Loss: 1.6026, Perplexity: 4.96598\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.6163, Perplexity: 13.6853\n",
      "Epoch [2/3], Step [38400/41412], Loss: 2.4721, Perplexity: 11.8467\n",
      "Epoch [2/3], Step [38500/41412], Loss: 1.9676, Perplexity: 7.15347\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.2133, Perplexity: 9.14623\n",
      "Epoch [2/3], Step [38700/41412], Loss: 2.0757, Perplexity: 7.96992\n",
      "Epoch [2/3], Step [38800/41412], Loss: 2.3561, Perplexity: 10.5498\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.0230, Perplexity: 7.56084\n",
      "Epoch [2/3], Step [39000/41412], Loss: 1.8932, Perplexity: 6.64043\n",
      "Epoch [2/3], Step [39100/41412], Loss: 2.6445, Perplexity: 14.0758\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.1295, Perplexity: 8.41098\n",
      "Epoch [2/3], Step [39300/41412], Loss: 1.7180, Perplexity: 5.57367\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.5175, Perplexity: 12.3972\n",
      "Epoch [2/3], Step [39500/41412], Loss: 1.8384, Perplexity: 6.28622\n",
      "Epoch [2/3], Step [39600/41412], Loss: 2.1891, Perplexity: 8.92744\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.0771, Perplexity: 7.98148\n",
      "Epoch [2/3], Step [39800/41412], Loss: 1.9871, Perplexity: 7.29442\n",
      "Epoch [2/3], Step [39900/41412], Loss: 1.9336, Perplexity: 6.91442\n",
      "Epoch [2/3], Step [40000/41412], Loss: 2.6866, Perplexity: 14.6811\n",
      "Epoch [2/3], Step [40100/41412], Loss: 2.6149, Perplexity: 13.6663\n",
      "Epoch [2/3], Step [40200/41412], Loss: 1.8000, Perplexity: 6.04970\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.6170, Perplexity: 13.6946\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.4520, Perplexity: 11.6110\n",
      "Epoch [2/3], Step [40500/41412], Loss: 2.5514, Perplexity: 12.8254\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.3304, Perplexity: 10.2824\n",
      "Epoch [2/3], Step [40700/41412], Loss: 2.6374, Perplexity: 13.9765\n",
      "Epoch [2/3], Step [40800/41412], Loss: 2.6134, Perplexity: 13.6448\n",
      "Epoch [2/3], Step [40900/41412], Loss: 1.9816, Perplexity: 7.25464\n",
      "Epoch [2/3], Step [41000/41412], Loss: 2.0718, Perplexity: 7.93908\n",
      "Epoch [2/3], Step [41100/41412], Loss: 2.3231, Perplexity: 10.2069\n",
      "Epoch [2/3], Step [41200/41412], Loss: 2.1380, Perplexity: 8.48240\n",
      "Epoch [2/3], Step [41300/41412], Loss: 2.0968, Perplexity: 8.14037\n",
      "Epoch [2/3], Step [41400/41412], Loss: 2.0512, Perplexity: 7.77712\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.3735, Perplexity: 10.735103\n",
      "Epoch [3/3], Step [200/41412], Loss: 1.9301, Perplexity: 6.890078\n",
      "Epoch [3/3], Step [300/41412], Loss: 1.5123, Perplexity: 4.53736\n",
      "Epoch [3/3], Step [400/41412], Loss: 1.9347, Perplexity: 6.92221\n",
      "Epoch [3/3], Step [500/41412], Loss: 1.6874, Perplexity: 5.40527\n",
      "Epoch [3/3], Step [600/41412], Loss: 2.0288, Perplexity: 7.60490\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.6604, Perplexity: 14.3026\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.3279, Perplexity: 10.2564\n",
      "Epoch [3/3], Step [900/41412], Loss: 1.9881, Perplexity: 7.30164\n",
      "Epoch [3/3], Step [1000/41412], Loss: 2.4718, Perplexity: 11.8443\n",
      "Epoch [3/3], Step [1100/41412], Loss: 2.0901, Perplexity: 8.08609\n",
      "Epoch [3/3], Step [1200/41412], Loss: 1.9098, Perplexity: 6.75188\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.3999, Perplexity: 11.0222\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.1882, Perplexity: 8.91919\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.3102, Perplexity: 10.0769\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.4973, Perplexity: 12.1493\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.2274, Perplexity: 9.27585\n",
      "Epoch [3/3], Step [1800/41412], Loss: 2.2657, Perplexity: 9.63792\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.2971, Perplexity: 9.94522\n",
      "Epoch [3/3], Step [2000/41412], Loss: 1.6971, Perplexity: 5.45805\n",
      "Epoch [3/3], Step [2100/41412], Loss: 3.1270, Perplexity: 22.8058\n",
      "Epoch [3/3], Step [2200/41412], Loss: 2.3941, Perplexity: 10.9579\n",
      "Epoch [3/3], Step [2300/41412], Loss: 2.5621, Perplexity: 12.9624\n",
      "Epoch [3/3], Step [2400/41412], Loss: 1.7860, Perplexity: 5.96583\n",
      "Epoch [3/3], Step [2500/41412], Loss: 1.9094, Perplexity: 6.74880\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.4988, Perplexity: 12.1678\n",
      "Epoch [3/3], Step [2700/41412], Loss: 2.3665, Perplexity: 10.6602\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.3885, Perplexity: 10.8970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [2900/41412], Loss: 2.3042, Perplexity: 10.0159\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.3354, Perplexity: 10.3335\n",
      "Epoch [3/3], Step [3100/41412], Loss: 2.3506, Perplexity: 10.4916\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.5531, Perplexity: 12.8464\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.0737, Perplexity: 7.95397\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.4406, Perplexity: 11.4797\n",
      "Epoch [3/3], Step [3500/41412], Loss: 2.3657, Perplexity: 10.6513\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.2091, Perplexity: 9.10730\n",
      "Epoch [3/3], Step [3700/41412], Loss: 1.8170, Perplexity: 6.15374\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.0686, Perplexity: 7.91392\n",
      "Epoch [3/3], Step [3900/41412], Loss: 2.5632, Perplexity: 12.9769\n",
      "Epoch [3/3], Step [4000/41412], Loss: 1.8842, Perplexity: 6.58115\n",
      "Epoch [3/3], Step [4100/41412], Loss: 2.0249, Perplexity: 7.57507\n",
      "Epoch [3/3], Step [4200/41412], Loss: 1.9806, Perplexity: 7.24702\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.5231, Perplexity: 12.4673\n",
      "Epoch [3/3], Step [4400/41412], Loss: 2.0297, Perplexity: 7.61176\n",
      "Epoch [3/3], Step [4500/41412], Loss: 1.9527, Perplexity: 7.04806\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.0548, Perplexity: 7.80502\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.5803, Perplexity: 13.2013\n",
      "Epoch [3/3], Step [4800/41412], Loss: 2.1697, Perplexity: 8.75535\n",
      "Epoch [3/3], Step [4900/41412], Loss: 1.9823, Perplexity: 7.25972\n",
      "Epoch [3/3], Step [5000/41412], Loss: 2.1178, Perplexity: 8.31301\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.1359, Perplexity: 8.46466\n",
      "Epoch [3/3], Step [5200/41412], Loss: 2.2582, Perplexity: 9.56624\n",
      "Epoch [3/3], Step [5300/41412], Loss: 2.1126, Perplexity: 8.26969\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.5703, Perplexity: 13.0697\n",
      "Epoch [3/3], Step [5500/41412], Loss: 2.1942, Perplexity: 8.97254\n",
      "Epoch [3/3], Step [5600/41412], Loss: 1.8770, Perplexity: 6.53402\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.9295, Perplexity: 18.7192\n",
      "Epoch [3/3], Step [5800/41412], Loss: 2.5237, Perplexity: 12.4743\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.6189, Perplexity: 13.7201\n",
      "Epoch [3/3], Step [6000/41412], Loss: 1.7846, Perplexity: 5.95753\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.1194, Perplexity: 8.32591\n",
      "Epoch [3/3], Step [6200/41412], Loss: 2.0978, Perplexity: 8.14843\n",
      "Epoch [3/3], Step [6300/41412], Loss: 2.0590, Perplexity: 7.83857\n",
      "Epoch [3/3], Step [6400/41412], Loss: 2.5860, Perplexity: 13.2767\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.6111, Perplexity: 13.6142\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.0117, Perplexity: 7.47626\n",
      "Epoch [3/3], Step [6700/41412], Loss: 1.9145, Perplexity: 6.78380\n",
      "Epoch [3/3], Step [6800/41412], Loss: 2.1688, Perplexity: 8.74780\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.7095, Perplexity: 15.0210\n",
      "Epoch [3/3], Step [7000/41412], Loss: 1.8990, Perplexity: 6.67952\n",
      "Epoch [3/3], Step [7100/41412], Loss: 2.7596, Perplexity: 15.7933\n",
      "Epoch [3/3], Step [7200/41412], Loss: 2.7108, Perplexity: 15.0412\n",
      "Epoch [3/3], Step [7300/41412], Loss: 2.1833, Perplexity: 8.87546\n",
      "Epoch [3/3], Step [7400/41412], Loss: 2.5770, Perplexity: 13.1579\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.5803, Perplexity: 13.2013\n",
      "Epoch [3/3], Step [7600/41412], Loss: 1.9191, Perplexity: 6.81507\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.0616, Perplexity: 7.85854\n",
      "Epoch [3/3], Step [7800/41412], Loss: 2.1862, Perplexity: 8.90177\n",
      "Epoch [3/3], Step [7900/41412], Loss: 1.6228, Perplexity: 5.06733\n",
      "Epoch [3/3], Step [8000/41412], Loss: 2.1506, Perplexity: 8.59037\n",
      "Epoch [3/3], Step [8100/41412], Loss: 2.4561, Perplexity: 11.6589\n",
      "Epoch [3/3], Step [8200/41412], Loss: 1.6033, Perplexity: 4.96967\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.3942, Perplexity: 10.9599\n",
      "Epoch [3/3], Step [8400/41412], Loss: 1.9314, Perplexity: 6.89932\n",
      "Epoch [3/3], Step [8500/41412], Loss: 2.6402, Perplexity: 14.0156\n",
      "Epoch [3/3], Step [8600/41412], Loss: 1.9586, Perplexity: 7.08910\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.4049, Perplexity: 11.0773\n",
      "Epoch [3/3], Step [8800/41412], Loss: 2.2047, Perplexity: 9.06737\n",
      "Epoch [3/3], Step [8900/41412], Loss: 1.8883, Perplexity: 6.60804\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.5674, Perplexity: 13.0321\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.2606, Perplexity: 9.58921\n",
      "Epoch [3/3], Step [9200/41412], Loss: 2.0173, Perplexity: 7.51812\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.1213, Perplexity: 8.34213\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.4066, Perplexity: 11.0964\n",
      "Epoch [3/3], Step [9500/41412], Loss: 2.0296, Perplexity: 7.610803\n",
      "Epoch [3/3], Step [9600/41412], Loss: 2.2365, Perplexity: 9.36064\n",
      "Epoch [3/3], Step [9700/41412], Loss: 2.8764, Perplexity: 17.7503\n",
      "Epoch [3/3], Step [9800/41412], Loss: 1.8557, Perplexity: 6.39611\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.7042, Perplexity: 14.9424\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.0907, Perplexity: 8.0909\n",
      "Epoch [3/3], Step [10100/41412], Loss: 2.2264, Perplexity: 9.26673\n",
      "Epoch [3/3], Step [10200/41412], Loss: 2.3703, Perplexity: 10.7001\n",
      "Epoch [3/3], Step [10300/41412], Loss: 1.9138, Perplexity: 6.77862\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.1306, Perplexity: 8.42002\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.5233, Perplexity: 12.4702\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.7660, Perplexity: 15.8950\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.7073, Perplexity: 14.9881\n",
      "Epoch [3/3], Step [10800/41412], Loss: 1.9555, Perplexity: 7.06718\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.2654, Perplexity: 9.63508\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.4331, Perplexity: 11.3945\n",
      "Epoch [3/3], Step [11100/41412], Loss: 2.2239, Perplexity: 9.24328\n",
      "Epoch [3/3], Step [11200/41412], Loss: 1.8228, Perplexity: 6.18936\n",
      "Epoch [3/3], Step [11300/41412], Loss: 1.8481, Perplexity: 6.34763\n",
      "Epoch [3/3], Step [11400/41412], Loss: 2.1553, Perplexity: 8.63036\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.3935, Perplexity: 10.9517\n",
      "Epoch [3/3], Step [11600/41412], Loss: 1.9720, Perplexity: 7.18514\n",
      "Epoch [3/3], Step [11700/41412], Loss: 3.1105, Perplexity: 22.4324\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.1667, Perplexity: 8.72928\n",
      "Epoch [3/3], Step [11900/41412], Loss: 1.6317, Perplexity: 5.11267\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.7900, Perplexity: 16.2816\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.1689, Perplexity: 8.74882\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.2187, Perplexity: 9.19577\n",
      "Epoch [3/3], Step [12300/41412], Loss: 1.6117, Perplexity: 5.01146\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.5676, Perplexity: 13.0351\n",
      "Epoch [3/3], Step [12500/41412], Loss: 1.7467, Perplexity: 5.73575\n",
      "Epoch [3/3], Step [12600/41412], Loss: 2.5806, Perplexity: 13.2056\n",
      "Epoch [3/3], Step [12700/41412], Loss: 2.5766, Perplexity: 13.1522\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.1972, Perplexity: 8.99944\n",
      "Epoch [3/3], Step [12900/41412], Loss: 1.9783, Perplexity: 7.23032\n",
      "Epoch [3/3], Step [13000/41412], Loss: 2.6538, Perplexity: 14.2079\n",
      "Epoch [3/3], Step [13100/41412], Loss: 1.9556, Perplexity: 7.06808\n",
      "Epoch [3/3], Step [13200/41412], Loss: 1.8175, Perplexity: 6.15675\n",
      "Epoch [3/3], Step [13300/41412], Loss: 1.7347, Perplexity: 5.66743\n",
      "Epoch [3/3], Step [13400/41412], Loss: 2.3770, Perplexity: 10.7728\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.2908, Perplexity: 9.88248\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.2473, Perplexity: 9.46183\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.5693, Perplexity: 13.0572\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.2229, Perplexity: 9.23419\n",
      "Epoch [3/3], Step [13900/41412], Loss: 2.5710, Perplexity: 13.0794\n",
      "Epoch [3/3], Step [14000/41412], Loss: 1.8422, Perplexity: 6.31047\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.7048, Perplexity: 14.9516\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.4657, Perplexity: 11.7723\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.2868, Perplexity: 9.84309\n",
      "Epoch [3/3], Step [14400/41412], Loss: 2.0893, Perplexity: 8.07930\n",
      "Epoch [3/3], Step [14500/41412], Loss: 2.4066, Perplexity: 11.0957\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.4046, Perplexity: 11.0741\n",
      "Epoch [3/3], Step [14700/41412], Loss: 2.1600, Perplexity: 8.67141\n",
      "Epoch [3/3], Step [14800/41412], Loss: 1.9524, Perplexity: 7.04567\n",
      "Epoch [3/3], Step [14900/41412], Loss: 2.2795, Perplexity: 9.77158\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.4696, Perplexity: 11.8182\n",
      "Epoch [3/3], Step [15100/41412], Loss: 1.9229, Perplexity: 6.84062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [15200/41412], Loss: 2.5274, Perplexity: 12.5205\n",
      "Epoch [3/3], Step [15300/41412], Loss: 2.0735, Perplexity: 7.95255\n",
      "Epoch [3/3], Step [15400/41412], Loss: 1.9697, Perplexity: 7.16869\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.2015, Perplexity: 9.03868\n",
      "Epoch [3/3], Step [15600/41412], Loss: 2.0370, Perplexity: 7.66758\n",
      "Epoch [3/3], Step [15700/41412], Loss: 2.6438, Perplexity: 14.0665\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.1839, Perplexity: 8.88050\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.3885, Perplexity: 10.8971\n",
      "Epoch [3/3], Step [16000/41412], Loss: 1.9395, Perplexity: 6.95556\n",
      "Epoch [3/3], Step [16100/41412], Loss: 2.2560, Perplexity: 9.54461\n",
      "Epoch [3/3], Step [16200/41412], Loss: 1.6659, Perplexity: 5.29050\n",
      "Epoch [3/3], Step [16300/41412], Loss: 2.6780, Perplexity: 14.5555\n",
      "Epoch [3/3], Step [16400/41412], Loss: 2.2983, Perplexity: 9.95720\n",
      "Epoch [3/3], Step [16500/41412], Loss: 1.9018, Perplexity: 6.69806\n",
      "Epoch [3/3], Step [16600/41412], Loss: 2.2835, Perplexity: 9.81128\n",
      "Epoch [3/3], Step [16700/41412], Loss: 1.9857, Perplexity: 7.28437\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.1720, Perplexity: 8.77551\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.3473, Perplexity: 10.4573\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.4773, Perplexity: 11.9087\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.1131, Perplexity: 8.27421\n",
      "Epoch [3/3], Step [17200/41412], Loss: 1.8291, Perplexity: 6.22827\n",
      "Epoch [3/3], Step [17300/41412], Loss: 1.9375, Perplexity: 6.94140\n",
      "Epoch [3/3], Step [17400/41412], Loss: 2.1990, Perplexity: 9.01621\n",
      "Epoch [3/3], Step [17500/41412], Loss: 2.1531, Perplexity: 8.61161\n",
      "Epoch [3/3], Step [17600/41412], Loss: 2.8598, Perplexity: 17.4573\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.5479, Perplexity: 12.7807\n",
      "Epoch [3/3], Step [17800/41412], Loss: 2.2462, Perplexity: 9.45225\n",
      "Epoch [3/3], Step [17900/41412], Loss: 2.1906, Perplexity: 8.94102\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.4329, Perplexity: 11.3915\n",
      "Epoch [3/3], Step [18100/41412], Loss: 1.6765, Perplexity: 5.34666\n",
      "Epoch [3/3], Step [18200/41412], Loss: 1.9185, Perplexity: 6.81080\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.2388, Perplexity: 9.38228\n",
      "Epoch [3/3], Step [18400/41412], Loss: 1.8527, Perplexity: 6.37725\n",
      "Epoch [3/3], Step [18500/41412], Loss: 2.2667, Perplexity: 9.64737\n",
      "Epoch [3/3], Step [18600/41412], Loss: 2.1090, Perplexity: 8.23994\n",
      "Epoch [3/3], Step [18700/41412], Loss: 1.8772, Perplexity: 6.53554\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.1129, Perplexity: 8.27214\n",
      "Epoch [3/3], Step [18900/41412], Loss: 2.2393, Perplexity: 9.38645\n",
      "Epoch [3/3], Step [19000/41412], Loss: 2.1956, Perplexity: 8.98562\n",
      "Epoch [3/3], Step [19100/41412], Loss: 2.2354, Perplexity: 9.35037\n",
      "Epoch [3/3], Step [19200/41412], Loss: 1.7963, Perplexity: 6.02767\n",
      "Epoch [3/3], Step [19300/41412], Loss: 2.0738, Perplexity: 7.95474\n",
      "Epoch [3/3], Step [19400/41412], Loss: 2.0393, Perplexity: 7.68562\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.0990, Perplexity: 8.15817\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.1602, Perplexity: 8.67251\n",
      "Epoch [3/3], Step [19700/41412], Loss: 2.5814, Perplexity: 13.2150\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.8428, Perplexity: 17.1638\n",
      "Epoch [3/3], Step [19900/41412], Loss: 2.3701, Perplexity: 10.6983\n",
      "Epoch [3/3], Step [20000/41412], Loss: 1.7052, Perplexity: 5.50286\n",
      "Epoch [3/3], Step [20100/41412], Loss: 3.2933, Perplexity: 26.9309\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.0477, Perplexity: 7.75036\n",
      "Epoch [3/3], Step [20300/41412], Loss: 1.6965, Perplexity: 5.45471\n",
      "Epoch [3/3], Step [20400/41412], Loss: 1.3959, Perplexity: 4.03870\n",
      "Epoch [3/3], Step [20500/41412], Loss: 3.4993, Perplexity: 33.0935\n",
      "Epoch [3/3], Step [20600/41412], Loss: 1.9555, Perplexity: 7.06770\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.2001, Perplexity: 9.02573\n",
      "Epoch [3/3], Step [20800/41412], Loss: 2.5146, Perplexity: 12.3618\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.1646, Perplexity: 8.71158\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.1255, Perplexity: 8.37686\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.1751, Perplexity: 8.80312\n",
      "Epoch [3/3], Step [21200/41412], Loss: 1.6724, Perplexity: 5.32525\n",
      "Epoch [3/3], Step [21300/41412], Loss: 1.8028, Perplexity: 6.06646\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.5112, Perplexity: 12.3194\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.7188, Perplexity: 15.1624\n",
      "Epoch [3/3], Step [21600/41412], Loss: 1.9592, Perplexity: 7.09381\n",
      "Epoch [3/3], Step [21700/41412], Loss: 2.3450, Perplexity: 10.4331\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.6191, Perplexity: 13.7231\n",
      "Epoch [3/3], Step [21900/41412], Loss: 2.3339, Perplexity: 10.3179\n",
      "Epoch [3/3], Step [22000/41412], Loss: 1.4637, Perplexity: 4.32202\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.4017, Perplexity: 11.0421\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.7474, Perplexity: 15.6021\n",
      "Epoch [3/3], Step [22300/41412], Loss: 2.1878, Perplexity: 8.91541\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.4150, Perplexity: 11.1895\n",
      "Epoch [3/3], Step [22500/41412], Loss: 1.8136, Perplexity: 6.13277\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.0130, Perplexity: 7.48576\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.5960, Perplexity: 13.4097\n",
      "Epoch [3/3], Step [22800/41412], Loss: 2.1192, Perplexity: 8.32479\n",
      "Epoch [3/3], Step [22900/41412], Loss: 1.7514, Perplexity: 5.76252\n",
      "Epoch [3/3], Step [23000/41412], Loss: 2.0981, Perplexity: 8.15096\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.2248, Perplexity: 9.25171\n",
      "Epoch [3/3], Step [23200/41412], Loss: 2.5169, Perplexity: 12.3898\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.1798, Perplexity: 8.84454\n",
      "Epoch [3/3], Step [23400/41412], Loss: 2.2711, Perplexity: 9.68988\n",
      "Epoch [3/3], Step [23500/41412], Loss: 1.9399, Perplexity: 6.95819\n",
      "Epoch [3/3], Step [23600/41412], Loss: 2.0388, Perplexity: 7.68118\n",
      "Epoch [3/3], Step [23700/41412], Loss: 2.0239, Perplexity: 7.56780\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.5475, Perplexity: 12.7754\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.5094, Perplexity: 12.2978\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.0323, Perplexity: 7.63163\n",
      "Epoch [3/3], Step [24100/41412], Loss: 2.5825, Perplexity: 13.2298\n",
      "Epoch [3/3], Step [24200/41412], Loss: 2.4025, Perplexity: 11.0503\n",
      "Epoch [3/3], Step [24300/41412], Loss: 2.0213, Perplexity: 7.54824\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.7829, Perplexity: 16.1660\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.4394, Perplexity: 11.4663\n",
      "Epoch [3/3], Step [24600/41412], Loss: 1.9603, Perplexity: 7.10144\n",
      "Epoch [3/3], Step [24700/41412], Loss: 1.8821, Perplexity: 6.56704\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.1553, Perplexity: 8.63088\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.0173, Perplexity: 7.51813\n",
      "Epoch [3/3], Step [25000/41412], Loss: 2.0052, Perplexity: 7.42745\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.2371, Perplexity: 9.36582\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.5517, Perplexity: 12.8286\n",
      "Epoch [3/3], Step [25300/41412], Loss: 2.7899, Perplexity: 16.2794\n",
      "Epoch [3/3], Step [25400/41412], Loss: 1.9807, Perplexity: 7.24792\n",
      "Epoch [3/3], Step [25500/41412], Loss: 3.1102, Perplexity: 22.4245\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.1467, Perplexity: 8.55630\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.5816, Perplexity: 13.2186\n",
      "Epoch [3/3], Step [25800/41412], Loss: 1.9360, Perplexity: 6.93084\n",
      "Epoch [3/3], Step [25900/41412], Loss: 2.4722, Perplexity: 11.8483\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.2912, Perplexity: 9.88722\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.0512, Perplexity: 7.77708\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.1283, Perplexity: 8.40086\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.1812, Perplexity: 8.85678\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.2265, Perplexity: 9.26731\n",
      "Epoch [3/3], Step [26500/41412], Loss: 2.2011, Perplexity: 9.03534\n",
      "Epoch [3/3], Step [26600/41412], Loss: 1.9988, Perplexity: 7.38032\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.1669, Perplexity: 8.73100\n",
      "Epoch [3/3], Step [26800/41412], Loss: 2.1961, Perplexity: 8.99029\n",
      "Epoch [3/3], Step [26900/41412], Loss: 2.1108, Perplexity: 8.25491\n",
      "Epoch [3/3], Step [27000/41412], Loss: 1.8271, Perplexity: 6.21552\n",
      "Epoch [3/3], Step [27100/41412], Loss: 1.8644, Perplexity: 6.45248\n",
      "Epoch [3/3], Step [27200/41412], Loss: 2.5127, Perplexity: 12.3385\n",
      "Epoch [3/3], Step [27300/41412], Loss: 1.9698, Perplexity: 7.16891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [27400/41412], Loss: 1.8046, Perplexity: 6.07769\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.1933, Perplexity: 8.96454\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.0592, Perplexity: 7.83988\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.1914, Perplexity: 8.94807\n",
      "Epoch [3/3], Step [27800/41412], Loss: 2.0452, Perplexity: 7.73099\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.0829, Perplexity: 8.02744\n",
      "Epoch [3/3], Step [28000/41412], Loss: 2.2732, Perplexity: 9.71002\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.2705, Perplexity: 9.68424\n",
      "Epoch [3/3], Step [28200/41412], Loss: 1.8850, Perplexity: 6.58665\n",
      "Epoch [3/3], Step [28300/41412], Loss: 1.8869, Perplexity: 6.59905\n",
      "Epoch [3/3], Step [28400/41412], Loss: 1.8704, Perplexity: 6.49072\n",
      "Epoch [3/3], Step [28500/41412], Loss: 2.3384, Perplexity: 10.3644\n",
      "Epoch [3/3], Step [28600/41412], Loss: 2.0671, Perplexity: 7.90227\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.8758, Perplexity: 17.7388\n",
      "Epoch [3/3], Step [28800/41412], Loss: 2.1731, Perplexity: 8.78577\n",
      "Epoch [3/3], Step [28900/41412], Loss: 1.9333, Perplexity: 6.91201\n",
      "Epoch [3/3], Step [29000/41412], Loss: 2.4968, Perplexity: 12.1433\n",
      "Epoch [3/3], Step [29100/41412], Loss: 2.2988, Perplexity: 9.96273\n",
      "Epoch [3/3], Step [29200/41412], Loss: 1.9152, Perplexity: 6.78853\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.2510, Perplexity: 9.49751\n",
      "Epoch [3/3], Step [29400/41412], Loss: 1.8754, Perplexity: 6.52344\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.3697, Perplexity: 10.6942\n",
      "Epoch [3/3], Step [29600/41412], Loss: 2.0814, Perplexity: 8.01563\n",
      "Epoch [3/3], Step [29700/41412], Loss: 1.7716, Perplexity: 5.88012\n",
      "Epoch [3/3], Step [29800/41412], Loss: 2.2696, Perplexity: 9.67580\n",
      "Epoch [3/3], Step [29900/41412], Loss: 2.0293, Perplexity: 7.60912\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.8253, Perplexity: 16.8657\n",
      "Epoch [3/3], Step [30100/41412], Loss: 1.9505, Perplexity: 7.03258\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.4549, Perplexity: 11.6455\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.5833, Perplexity: 13.2414\n",
      "Epoch [3/3], Step [30400/41412], Loss: 2.2365, Perplexity: 9.360623\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.2800, Perplexity: 9.77703\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.1664, Perplexity: 8.72689\n",
      "Epoch [3/3], Step [30700/41412], Loss: 2.3119, Perplexity: 10.0936\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.0347, Perplexity: 7.65030\n",
      "Epoch [3/3], Step [30900/41412], Loss: 1.8832, Perplexity: 6.57473\n",
      "Epoch [3/3], Step [31000/41412], Loss: 1.9756, Perplexity: 7.21080\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.0952, Perplexity: 8.12725\n",
      "Epoch [3/3], Step [31200/41412], Loss: 2.4271, Perplexity: 11.3256\n",
      "Epoch [3/3], Step [31300/41412], Loss: 1.8636, Perplexity: 6.44698\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.6960, Perplexity: 14.8208\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.6999, Perplexity: 14.8780\n",
      "Epoch [3/3], Step [31600/41412], Loss: 2.1894, Perplexity: 8.93027\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.2730, Perplexity: 9.70897\n",
      "Epoch [3/3], Step [31800/41412], Loss: 2.1119, Perplexity: 8.26396\n",
      "Epoch [3/3], Step [31900/41412], Loss: 2.3279, Perplexity: 10.2562\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.1602, Perplexity: 8.67257\n",
      "Epoch [3/3], Step [32100/41412], Loss: 1.8894, Perplexity: 6.61532\n",
      "Epoch [3/3], Step [32200/41412], Loss: 1.5025, Perplexity: 4.49270\n",
      "Epoch [3/3], Step [32300/41412], Loss: 2.7411, Perplexity: 15.5037\n",
      "Epoch [3/3], Step [32400/41412], Loss: 1.9754, Perplexity: 7.20973\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.3253, Perplexity: 10.2301\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.3998, Perplexity: 11.0206\n",
      "Epoch [3/3], Step [32700/41412], Loss: 2.1267, Perplexity: 8.38707\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.2516, Perplexity: 9.50343\n",
      "Epoch [3/3], Step [32900/41412], Loss: 2.2919, Perplexity: 9.89368\n",
      "Epoch [3/3], Step [33000/41412], Loss: 2.0986, Perplexity: 8.15452\n",
      "Epoch [3/3], Step [33100/41412], Loss: 2.9260, Perplexity: 18.6529\n",
      "Epoch [3/3], Step [33200/41412], Loss: 1.8338, Perplexity: 6.25751\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.6307, Perplexity: 13.8838\n",
      "Epoch [3/3], Step [33400/41412], Loss: 2.6173, Perplexity: 13.6993\n",
      "Epoch [3/3], Step [33500/41412], Loss: 2.6383, Perplexity: 13.9893\n",
      "Epoch [3/3], Step [33600/41412], Loss: 2.3548, Perplexity: 10.5363\n",
      "Epoch [3/3], Step [33700/41412], Loss: 1.9719, Perplexity: 7.18418\n",
      "Epoch [3/3], Step [33800/41412], Loss: 2.1083, Perplexity: 8.23431\n",
      "Epoch [3/3], Step [33900/41412], Loss: 2.2259, Perplexity: 9.26187\n",
      "Epoch [3/3], Step [34000/41412], Loss: 2.1508, Perplexity: 8.59162\n",
      "Epoch [3/3], Step [34100/41412], Loss: 2.2871, Perplexity: 9.84668\n",
      "Epoch [3/3], Step [34200/41412], Loss: 2.1738, Perplexity: 8.79206\n",
      "Epoch [3/3], Step [34300/41412], Loss: 2.3238, Perplexity: 10.2144\n",
      "Epoch [3/3], Step [34400/41412], Loss: 2.2441, Perplexity: 9.43209\n",
      "Epoch [3/3], Step [34500/41412], Loss: 2.2305, Perplexity: 9.30437\n",
      "Epoch [3/3], Step [34600/41412], Loss: 2.0151, Perplexity: 7.50170\n",
      "Epoch [3/3], Step [34700/41412], Loss: 2.3641, Perplexity: 10.6346\n",
      "Epoch [3/3], Step [34800/41412], Loss: 1.9417, Perplexity: 6.97067\n",
      "Epoch [3/3], Step [34900/41412], Loss: 1.7627, Perplexity: 5.82831\n",
      "Epoch [3/3], Step [35000/41412], Loss: 2.2359, Perplexity: 9.35518\n",
      "Epoch [3/3], Step [35100/41412], Loss: 2.4245, Perplexity: 11.2969\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.0350, Perplexity: 7.65248\n",
      "Epoch [3/3], Step [35300/41412], Loss: 1.8051, Perplexity: 6.08052\n",
      "Epoch [3/3], Step [35400/41412], Loss: 2.4521, Perplexity: 11.6131\n",
      "Epoch [3/3], Step [35500/41412], Loss: 2.9260, Perplexity: 18.6530\n",
      "Epoch [3/3], Step [35600/41412], Loss: 1.9741, Perplexity: 7.20024\n",
      "Epoch [3/3], Step [35700/41412], Loss: 2.7234, Perplexity: 15.2321\n",
      "Epoch [3/3], Step [35800/41412], Loss: 1.9116, Perplexity: 6.76382\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.0518, Perplexity: 7.78225\n",
      "Epoch [3/3], Step [36000/41412], Loss: 2.2779, Perplexity: 9.75654\n",
      "Epoch [3/3], Step [36100/41412], Loss: 1.8660, Perplexity: 6.46246\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.4443, Perplexity: 11.5227\n",
      "Epoch [3/3], Step [36300/41412], Loss: 2.3754, Perplexity: 10.7557\n",
      "Epoch [3/3], Step [36400/41412], Loss: 2.5906, Perplexity: 13.3376\n",
      "Epoch [3/3], Step [36500/41412], Loss: 2.0007, Perplexity: 7.39451\n",
      "Epoch [3/3], Step [36600/41412], Loss: 2.3360, Perplexity: 10.3403\n",
      "Epoch [3/3], Step [36700/41412], Loss: 1.9572, Perplexity: 7.07926\n",
      "Epoch [3/3], Step [36800/41412], Loss: 2.5584, Perplexity: 12.9154\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.0248, Perplexity: 7.57437\n",
      "Epoch [3/3], Step [37000/41412], Loss: 2.4827, Perplexity: 11.9739\n",
      "Epoch [3/3], Step [37100/41412], Loss: 2.2827, Perplexity: 9.80305\n",
      "Epoch [3/3], Step [37200/41412], Loss: 1.6480, Perplexity: 5.19654\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.3265, Perplexity: 10.2425\n",
      "Epoch [3/3], Step [37400/41412], Loss: 2.3358, Perplexity: 10.3376\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.5600, Perplexity: 12.9355\n",
      "Epoch [3/3], Step [37600/41412], Loss: 2.7029, Perplexity: 14.9227\n",
      "Epoch [3/3], Step [37700/41412], Loss: 2.9040, Perplexity: 18.2461\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.5387, Perplexity: 12.6637\n",
      "Epoch [3/3], Step [37900/41412], Loss: 2.4068, Perplexity: 11.0988\n",
      "Epoch [3/3], Step [38000/41412], Loss: 2.7549, Perplexity: 15.7199\n",
      "Epoch [3/3], Step [38100/41412], Loss: 1.8359, Perplexity: 6.27100\n",
      "Epoch [3/3], Step [38200/41412], Loss: 2.1679, Perplexity: 8.74037\n",
      "Epoch [3/3], Step [38300/41412], Loss: 2.2484, Perplexity: 9.47286\n",
      "Epoch [3/3], Step [38400/41412], Loss: 1.9481, Perplexity: 7.01504\n",
      "Epoch [3/3], Step [38500/41412], Loss: 2.1104, Perplexity: 8.25185\n",
      "Epoch [3/3], Step [38600/41412], Loss: 2.3910, Perplexity: 10.9246\n",
      "Epoch [3/3], Step [38700/41412], Loss: 2.0457, Perplexity: 7.73484\n",
      "Epoch [3/3], Step [38800/41412], Loss: 2.0080, Perplexity: 7.44833\n",
      "Epoch [3/3], Step [38900/41412], Loss: 1.8370, Perplexity: 6.27743\n",
      "Epoch [3/3], Step [39000/41412], Loss: 2.4381, Perplexity: 11.4516\n",
      "Epoch [3/3], Step [39100/41412], Loss: 2.5733, Perplexity: 13.1094\n",
      "Epoch [3/3], Step [39200/41412], Loss: 2.7298, Perplexity: 15.3306\n",
      "Epoch [3/3], Step [39300/41412], Loss: 2.5605, Perplexity: 12.9420\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.3374, Perplexity: 10.3541\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.6621, Perplexity: 14.3262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [39600/41412], Loss: 2.0497, Perplexity: 7.76577\n",
      "Epoch [3/3], Step [39700/41412], Loss: 2.0691, Perplexity: 7.91765\n",
      "Epoch [3/3], Step [39800/41412], Loss: 1.9019, Perplexity: 6.69898\n",
      "Epoch [3/3], Step [39900/41412], Loss: 2.3449, Perplexity: 10.4326\n",
      "Epoch [3/3], Step [40000/41412], Loss: 1.9205, Perplexity: 6.82446\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.2055, Perplexity: 9.07489\n",
      "Epoch [3/3], Step [40200/41412], Loss: 2.1693, Perplexity: 8.75248\n",
      "Epoch [3/3], Step [40300/41412], Loss: 2.5315, Perplexity: 12.5724\n",
      "Epoch [3/3], Step [40400/41412], Loss: 2.2750, Perplexity: 9.72827\n",
      "Epoch [3/3], Step [40500/41412], Loss: 1.8312, Perplexity: 6.24121\n",
      "Epoch [3/3], Step [40600/41412], Loss: 1.9426, Perplexity: 6.97683\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.2863, Perplexity: 9.83854\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.5843, Perplexity: 13.2539\n",
      "Epoch [3/3], Step [40900/41412], Loss: 2.2258, Perplexity: 9.26122\n",
      "Epoch [3/3], Step [41000/41412], Loss: 1.7707, Perplexity: 5.87500\n",
      "Epoch [3/3], Step [41100/41412], Loss: 2.0764, Perplexity: 7.97535\n",
      "Epoch [3/3], Step [41200/41412], Loss: 2.5624, Perplexity: 12.9672\n",
      "Epoch [3/3], Step [41300/41412], Loss: 2.0833, Perplexity: 8.03101\n",
      "Epoch [3/3], Step [41400/41412], Loss: 1.6288, Perplexity: 5.09757\n",
      "Epoch [3/3], Step [41412/41412], Loss: 1.7716, Perplexity: 5.88044"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from workspace_utils import active_session\n",
    "\n",
    "\n",
    "with active_session():\n",
    "\n",
    "# Open the training log file.\n",
    "    f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
